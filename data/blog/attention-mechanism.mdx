---
title: 'Attention Mechanism - What are Query, Key, and Value?'
date: '2023-01-22'
lastmod: '2023-01-22'
tags:
  [
    'Attention Mechanism',
    'Query, Key, Value and Output',
    'Additive Attention',
    'Bahdanau Attention',
    'Dot-Product Attention',
    'Scaled Dot-Product Attention',
    'Self-Attention',
    'Transformer',
  ]
draft: false
layout: PostSimple
summary: 'Explain Query, Key, Value and Output in Attention Mechanism with an analogy of Information Retrieval System. 
Discuss different Attention Mechanisms, including Additive/Bahdanau, Dot-Product, Scaled Dot-Product Attention, and Self Attention.
Talk about the Query, Key, Value, and Output choices in the standard Transformer.'
images: ['/static/images/attention/information-retrieval-fuzzy.png']
authors: ['default']
---

## Abstract

The Attention Mechanism is a flexible and powerful framework. However, it might be super confusing because of its flexibility. This
article explains the concept of `Query`, `Key`, `Value`, and `Output` with an analogy of an Information Retrieval System. This article
also shows how to achieve different Attention Mechanisms by varying:

1. Query-Key compatibility function
2. `Query`, `Key`, and `Value` choices

For example, the innovation of Self-Attention is to use the same vector as `Query`, `Key`, and `Value`, while the
innovation of Scaled Dot-Product Attention is to have a different Query-Key compatibility function.

<TOCInline exclude="Overview" toc={props.toc} asDisclosure />

## Information Retrieval System

An Information Retrieval System stores `Values` indexed by `Keys`. A `Query` is used to retrieve
related Key-Value pairs. The system returns an `Output` that is the associated `Value` or a transformed version of the associated `Value`.

### Exact Matching

<div className="text-center">
  <Image
    style={{ background: 'white' }}
    alt="Information Retrieval"
    src="/static/images/attention/information-retrieval.png"
    width="587"
    height="226"
  />
  <div>Fig.1 Finding the Exact Income by a Person's Name</div>
</div>

In Fig.1, we have a basic system that allows users to find the exact income by a person's name. The system stores the `Values` (Income) indexed by `Keys` (Name).
When a user enters a `Query` (Name), the system returns an `Output` (Income) where it is the `Value` (Income)
that is related to the `Query` (Name).

### Query-Key Compatibility

<div className="text-center">
  <Image
    style={{ background: 'white' }}
    alt="Information Retrieval (Fuzzy Matching)"
    src="/static/images/attention/information-retrieval-fuzzy.png"
    width="646"
    height="225"
  />
  <div>Fig.2 Estimating a Person's Income by His/Her Name</div>
</div>

Sometimes, we might not want an exact matching between `Query` and `Key`. For example, in Fig.2,
we want to estimate a person's income by their name. The `Query` might not be the same as the `Key` (Name).
In this case, the system computes Query-Key compatibility scores between the `Query` and all the `Keys` (Name) in the system.

We can use different functions to measure compatibility. The compatibility function takes the `Query` and `Key` as inputs and gives a score.

Note: Query-Key compatibility has also been called similarity, relevancy, and alignment in different places.

### Retrieval Output - Weighted Sum of Values

The returned `Output` is the weighted sum of all `Values`,
which is weighted by the Query-Key compatibility scores.

For example, in Fig.2,

1. If the scores are $[0.9, 0.1, 0.0]$ respectively, the returned `Output`
   will be $1000 \times 0.9 + 2000 \times 0.1 + 3000 \times 0.0 = 1100$.

2. If the scores are $[1.0, 0.0, 0.0]$ respectively, the returned `Output`
   will be $1000$, the same as retrieving the `Value` with the highest associated compatibility score.

### Information Retrieval in Sequential Data

We have discussed the Query-Key compatibility in terms of an Information Retrieval System, but how can we apply it into
Deep Learning? We can use different `Querys`, `Keys`, and `Values` choices for various NLP tasks. Let's
consider a simple task below:

For a 5-word sentence: **"i like to eat apple"**, we want to know if the word **"apple"** is a fruit or a company.

Let's have the following `Querys`, `Keys`, and `Values` choices:

- `Keys` = `Values` as five embedding vectors of each word = $\mathbf{k}_{1 \le s \le 5}$
- `Query` as the embedding vector of the word **"apple"** = $\mathbf{q}$

Note: This configuration is the same as the Self-Attention (Will be discussed below).

To understand if the word **"apple"** is a fruit or a company,
we need to know the context of the sentence. We can compute compatibility scores to determine which words we should focus on to decide if
the word **"apple"** is a fruit or a company.

Ideally, we want the `Key` **"eat"** to have a higher score than the other `Keys`
when `Query` is **"apple"**, for example $(\mathbf{q}, \mathbf{k}_4)$ should be
more compatible than $(\mathbf{q}, \mathbf{k}_3)$.

Let's make up some scores such that only the `Keys` **"eat"** and **"apple"** have a high score with the `Query` **"apple"**.:

<div style={{ overflowX: 'auto' }}>$$ [0.0, 0.0, 0.0, 0.4, 0.6] $$</div>

Then the `Output` for the `Query` **"apple"** will be the weighted sum of all `Values`:

<div style={{ overflowX: 'auto' }}>
  $$ \mathbf{o}=0.4 \times \mathbf{k}_4 + 0.6 \times \mathbf{k}_5 $$
</div>

## The General Form of Attention Mechanism

> An attention function can be described as mapping a query and a set of key-value pairs to an output,
> where the query, keys, values, and output are all vectors. The output is computed as a weighted sum
> of the values, where the weight assigned to each value is computed by a compatibility function of the
> query with the corresponding key. -- Attention Is All You Need[^1]

### Query, Key, Value, and Output Vectors

In the setting of deep learning, we have a `Query` vector $\mathbf{q}$, a `Key` vector $\mathbf{k}$, and a `Value` vector $\mathbf{v}$.
They are in their own spaces, that's $\mathbf{q} \in \R^{d_q}$, $\mathbf{k} \in \R^{d_k}$ and $\mathbf{v} \in \R^{d_v}$.
The returned output vector $\mathbf{o}$ is in the `Value` space, that's $\mathbf{o} \in \R^{d_v}$.

Recall the Key-Value setting for Sequential Data above; let's say there is a sentence with length $S$, then there will be $S$ `Key` vectors
and $S$ `Value` vectors. We can represent them as two matrices:

<div style={{overflowX: "auto"}}>
$$
\mathbf{K} \in \R^{S \times d_k} = [\mathbf{k}_1^T, \mathbf{k}_2^T...\mathbf{k}_S^T]\\
\mathbf{V} \in \R^{S \times d_v} = [\mathbf{v}_1^T, \mathbf{v}_2^T...\mathbf{v}_S^T]
$$
</div>

### Query-key Compatibility Function

A Query-Key compatibility function $f_{\mathbf{compatibility}}: (\R^{d_q}, \R^{d_k}) \to \R$
maps a `Query` vector and a `Key` vector to a score scaler. The higher the score, the more compatible the `Query` vector and `Key` vector are.
We will have $S$ compatibility scores:

<div style={{overflowX: "auto"}}>
$$
\forall 1 \le s \le S: e_s \in \R = f_{\mathbf{compatibility}}(\mathbf{q}, \mathbf{k}_s)
$$
</div>

### Attention Weights and the Output Vectors

The attention weights are usually defined by applying softmax to the compatibility scores, such that the sum of the weights is 1.
Here is how we compute these $S$ attention weights:

<div style={{overflowX: "auto"}}>
$$
\forall 1 \le s \le S: a_s \in \R = \frac{\exp(e_s)}{\sum_{s=1}^S \exp(e_s)}
$$
</div>

Then the `Output` vector $\mathbf{o} \in \R^{d_v}$ is the weighted sum of those $S$ `Value` vectors:

<div style={{ overflowX: 'auto' }}>
  $$ \mathbf{o} \in \R^{d_v} = \sum_s^S a_s \mathbf{v}_s $$
</div>

### Pytorch Implementation of an Abstract Attention Module

Here we implement an abstract attention module in Pytorch. The `compute_compatibility_score` method is the
compatibility function $f_{\mathbf{compatibility}}$. The whole implementation is based on the matrix form of the attention mechanism.
We assume the second-last of the tensors is the sequence length, and the last is the embedding dimension.

```python
import torch

class AbstractAttention(torch.nn.Module):
  """
  We assume the second-last of the tensors is the sequence length,
  and the last is the embedding dimension.
  """

  def __init__(self):
    super().__init__()

  def compute_compatibility_score(self, query: torch.Tensor, key: torch.Tensor):
    """
    Args:
      key (torch.Tensor): The Key tensor.
      query (torch.Tensor): The Query.
    Return:
      torch.Tensor: The score tensor with the last two dimensions equals the sequence length.
    """
    raise NotImplemented

  def forward(self, value: torch.Tensor, key: torch.Tensor, query: torch.Tensor):
    """
    Args:
      value (torch.Tensor): The Value tensor.
      key (torch.Tensor): The Key tensor.
      query (torch.Tensor): The Query.
    Return:
      torch.Tensor: The Output tensor with the last two dimensions is the same as the Value tensor.
    """

    # The attentions are among the second last dimension.
    seq_len = value.size()[-2]
    assert value.size()[-2] == key.size()[-2] == query.size()[-2]

    # Compute the compatibility scores. The scoring is based on the last
    # dimension.
    compatibility_scores = self.compute_compatibility_score(query, key)
    assert compatibility_scores.size()[-2:] == (seq_len, seq_len)

    # Compute softmax over the second last dimension
    attention_weights = torch.nn.functional.softmax(
        compatibility_scores, dim=-1
    )
    assert attention_weights.size()[-2:] == (seq_len, seq_len)

    # Compute matrix multiplication over the last two dimensions
    output = torch.matmul(attention_weights, value)
    assert output.size() == value.size()

    return output
```

## Different Query-Key Compatibility Functions

### Additive / Bahdanau Attention

Additive Attention refers to a compatibility function approximated by a neural network. [^2]
It is a direct approximation approach to get a compatibility function. Because in Additive Attention,
we concatenate the `Query` vector and `Key` vector directly to feed into that neural network.

<div style={{ overflowX: 'auto' }}>
$$ 
f_{\mathbf{compatibility}}(\mathbf{q}, \mathbf{k}) \in \R = \mathbf{w}_{\mathbf{add}}^T \tanh(\mathbf{W}_{\mathbf{add}}[\mathbf{q}; \mathbf{k}])
$$
</div>

where

- $[\mathbf{q}; \mathbf{k}] \in \R^{d_q + d_k}$ is the concatenation of the `Query` vector and `Key` vector.
- $\mathbf{W}_{\mathbf{add}} \in \R^{d_{\mathbf{hidden}} \times (d_q + d_k)}$ and $\mathbf{w}_{\mathbf{add}} \in \R^{d_{\mathbf{hidden}}}$ are the
  trainable parameters of the neural network.

```python
# Matrix Form Pytorch Implementation of Additive Attention
class AdditiveAttention(AbstractAttention):
  def __init__(self, key_dim: int, query_dim: int, hidden_dim: int):
    """
    Args:
      key_dim (int): The embedding dimension of the Value tensor.
      query_dim (int): The embedding dimension of the Value tensor.
      hidden_dim (int): this controls the size of the hidden tensor and the
        size of the learnable weights.
    """
    super().__init__()

    self.linear = torch.nn.Linear(query_dim + key_dim, hidden_dim)
    self.w = torch.nn.Parameter(torch.Tensor(hidden_dim, 1))

  def compute_compatibility_score(self, query: torch.Tensor, key: torch.Tensor):
    seq_len = query.size()[-2]
    # Concat the embeddings of the Query tensor and Key tensor
    concated_tensor = torch.concat((query, key), dim=-1)
    # Feed a 1-layer mlp
    hidden_tensor = torch.tanh(
        self.linear(concated_tensor)
    )
    # Multipy with another parameter
    return torch.matmul(hidden_tensor, self.w.repeat(1, seq_len))

value_dim = 10
key_dim = 5
query_dim = 2
hidden_dim = 4
attention = AdditiveAttention(key_dim, query_dim, hidden_dim)

value = torch.rand(100, 20, value_dim)
key = torch.rand(100, 20, key_dim)
query = torch.rand(100, 20, query_dim)
out = attention(value, key, query)

out.size() # torch.Size([100, 20, 10])
```

### Dot-Product Attention and Scaled Dot-Product Attention

Regarding the angular similarity between two vectors, Dot-Product is a common choice. Dot-Product Attention refers to having the compatibility function as
the dot-product of the `Query` vector and `Key` vector. [^3]

<div style={{ overflowX: 'auto' }}>
$$ 
f_{\mathbf{compatibility}}(\mathbf{q}, \mathbf{k}) \in \R = \mathbf{q}^T \mathbf{k}
$$
</div>

Dot-Product Attention is the simplest compatibility measure form, as there are no hidden learning weights in the compatibility function.
However, this method assumes both `Query` and `Key` vectors have the same dimension.

```python
# Matrix Form Pytorch Implementation of Dot-Product Attention
class DotProductAttention(AbstractAttention):
  def __init__(self):
    super().__init__()

  def compute_compatibility_score(self, query: torch.Tensor, key: torch.Tensor):
    return torch.matmul(query, torch.transpose(key, -1, -2))

attention = DotProductAttention()

value = torch.rand(100, 20, 10)
key = torch.rand(100, 20, 5)
query = torch.rand(100, 20, 5)
out = attention(value, key, query)

out.size() # torch.Size([100, 20, 10])
```

Scaled Dot-Product Attention is a variant of Dot-Product Attention, where we scale the dot-product by a factor $\frac{1}{\sqrt{d_k}}$. [^4]

<div style={{ overflowX: 'auto' }}>
$$ 
f_{\mathbf{compatibility}}(\mathbf{q}, \mathbf{k}) \in \R = \frac{\mathbf{q}^T \mathbf{k}}{\sqrt{d_k}}
$$
</div>

Note: The original Transformer paper [^4] shows the matrix form of the scaled dot-product attention.
The formula above is the vector form that takes two vectors instead of matrices.

```python
# Matrix Form Pytorch Implementation of Scaled Dot-Product Attention
class ScaledDotProductAttention(AbstractAttention):
  def __init__(self):
    super().__init__()

  def compute_compatibility_score(self, query: torch.Tensor, key: torch.Tensor):
    return torch.matmul(query, torch.transpose(key, -1, -2)) / query.size()[-1]

attention = ScaledDotProductAttention()

value = torch.rand(100, 20, 10)
key = torch.rand(100, 20, 5)
query = torch.rand(100, 20, 5)
out = attention(value, key, query)

out.size() # torch.Size([100, 20, 10])
```

### Parametric vs Non-Parametric Compatibility Functions

We have discussed a Parametric compatibility function (Additive Attention) and a Non-Parametric compatibility function (Dot-Product Attention).
Parametric compatibility functions are more flexible, as they don't have an assumption on the `Query` and `Key` vectors' dimension.

<div className="text-center">
  <Image
    style={{ background: 'white' }}
    alt="Linear Projections Before Dot-Product"
    src="/static/images/attention/linear-projections-before-dot-product.png"
    width="320"
    height="440"
  />
  <div>Fig.3 Linear Projections Before Dot-Product in Transformer</div>
</div>

However, we can also apply linear projections to the `Query`, `Key`, and `Value` vectors
before applying a non-parametric compatibility function. This projects the `Query`, `Key`, and `Value` vectors into the same dimension.
Fig.3 shows the linear projections before the scaled dot-product in the standard Transformer model.

## Different Query, Key, Value Choices

Other than varying the Query-Key compatibility function, we can also change the `Query`, `Key`, and `Value` choices.
We can combine different compatibility functions and `Query`, `Key`, and `Value` choices
to construct other attention mechanisms. [^5]

For example, in Transformer, we have Self-Attention with Scaled Dot-Product Compatibility Function.

<div className="text-center">
  <Image
    style={{ background: 'white' }}
    alt="Attention Layers in Transformer"
    src="/static/images/attention/attention-layers-in-transformer.png"
    width="533"
    height="390"
  />
  <div>Fig.4 Different Attention Layers in Transformer</div>
</div>

We can examine different `Query`, `Key`, and `Value` choices in the standard Transformer model in Fig.4.

### Self-Attention [Fig.4 - 1]

When we have the same `Query` vectors, `Key` vectors, and `Value` vectors, we call it self-attention. The example
stated in the "Information Retrieval in Sequential Data" section above is a self-attention example.

Self-Attention has the following choices:

- `Keys` = `Values` are embedding vectors of each word in a sequence = $\mathbf{k}_{1 \le s \le S}$
- `Query` is an embedding vector of a word in the same sequence = $\mathbf{q}_{1 \le s \le S}$

```python
# Matrix Form Pytorch Implementation of Self-Attention
class ScaledDotProductAttention(AbstractAttention):
  def __init__(self):
    super().__init__()

  def compute_compatibility_score(self, query: torch.Tensor, key: torch.Tensor):
    return torch.matmul(query, torch.transpose(key, -1, -2)) / query.size()[-1]

attention = ScaledDotProductAttention()

value = torch.rand(100, 20, 10)
out = attention(value, value, value)

out.size() # torch.Size([100, 20, 10])
```

### Masked Self-Attention [Fig.4 - 2]

When we have "future" information in a sequence, we might want to mask them to prevent the model from cheating.

Masked Self-Attention has the following choices:

- `Keys` = `Values` are embedding vectors of each "seen" word in a sequence = $\mathbf{k}_{1 \le s1 \le s2}$
- `Query` is an embedding vector of a word in the same sequence = $\mathbf{q}_{1 \le s2 \le S}$

### Encoder-Decoder [Fig.4 - 3]

When we want to use a source sequence to predict a target sequence, it is common to see an Encoder-Decoder pattern.
In Fig.3 we can see the connection between the Encoder and the Decoder is an Attention layer. That attention layer has the
following choices:

- `Keys` = `Values` are embedding vectors of each word in the encoded source sequence.
- `Query` is an embedding vector of a word in the target Sequence

[^1]: [[Paper] Attention Is All You Need](https://arxiv.org/abs/1706.03762)
[^2]: [[Website] Paper with code - Additive Attention](https://paperswithcode.com/method/additive-attention)
[^3]: [[Website] Paper with code - Dot-Product Attention](https://paperswithcode.com/method/dot-product-attention)
[^4]: [[Website] Paper with code - Scaled Dot-Product Attention](https://paperswithcode.com/method/scaled)
[^5]: [[Website] Attention? Attention! | Lil'Log](https://lilianweng.github.io/posts/2018-06-24-attention/)
